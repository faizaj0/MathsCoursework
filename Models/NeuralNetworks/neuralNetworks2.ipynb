{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset kryptonite-24\n",
      "Epoch [10/500], Train Loss: 0.6923, Val Loss: 0.6938, Val Accuracy: 0.4981\n",
      "Epoch [20/500], Train Loss: 0.6924, Val Loss: 0.6949, Val Accuracy: 0.4956\n",
      "Epoch [30/500], Train Loss: 0.6917, Val Loss: 0.6946, Val Accuracy: 0.5041\n",
      "Epoch [40/500], Train Loss: 0.6895, Val Loss: 0.6959, Val Accuracy: 0.5049\n",
      "Epoch [50/500], Train Loss: 0.6881, Val Loss: 0.6957, Val Accuracy: 0.5046\n",
      "Epoch [60/500], Train Loss: 0.6848, Val Loss: 0.7022, Val Accuracy: 0.4991\n",
      "Epoch [70/500], Train Loss: 0.6798, Val Loss: 0.7037, Val Accuracy: 0.5012\n",
      "Epoch [80/500], Train Loss: 0.6732, Val Loss: 0.7066, Val Accuracy: 0.5019\n",
      "Epoch [90/500], Train Loss: 0.6691, Val Loss: 0.7093, Val Accuracy: 0.5041\n",
      "Epoch [100/500], Train Loss: 0.6606, Val Loss: 0.7113, Val Accuracy: 0.5058\n",
      "Epoch [110/500], Train Loss: 0.6562, Val Loss: 0.7277, Val Accuracy: 0.4988\n",
      "Epoch [120/500], Train Loss: 0.6501, Val Loss: 0.7190, Val Accuracy: 0.5010\n",
      "Epoch [130/500], Train Loss: 0.6436, Val Loss: 0.7342, Val Accuracy: 0.4992\n",
      "Epoch [140/500], Train Loss: 0.6355, Val Loss: 0.7363, Val Accuracy: 0.5021\n",
      "Epoch [150/500], Train Loss: 0.6264, Val Loss: 0.7375, Val Accuracy: 0.5033\n",
      "Epoch [160/500], Train Loss: 0.6222, Val Loss: 0.7541, Val Accuracy: 0.4972\n",
      "Epoch [170/500], Train Loss: 0.6167, Val Loss: 0.7524, Val Accuracy: 0.5006\n",
      "Epoch [180/500], Train Loss: 0.6096, Val Loss: 0.7478, Val Accuracy: 0.5058\n",
      "Epoch [190/500], Train Loss: 0.6056, Val Loss: 0.7566, Val Accuracy: 0.5034\n",
      "Epoch [200/500], Train Loss: 0.5978, Val Loss: 0.7726, Val Accuracy: 0.5027\n",
      "Epoch [210/500], Train Loss: 0.5981, Val Loss: 0.7765, Val Accuracy: 0.5062\n",
      "Epoch [220/500], Train Loss: 0.5895, Val Loss: 0.7703, Val Accuracy: 0.4950\n",
      "Epoch [230/500], Train Loss: 0.5815, Val Loss: 0.7751, Val Accuracy: 0.5094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m input_size \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m model, train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m train_model(X_train, y_train, X_val, y_val, input_size)\n\u001b[1;32m    197\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_test, y_test)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Plot metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_val, y_val, input_size, num_epochs, learning_rate, patience)\u001b[0m\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m    113\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[0;32m--> 114\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    115\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    116\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Accumulate training loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define Xavier initialization function\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)  # Initialize biases to zero for consistency\n",
    "\n",
    "# Define a single ResNet Block with Batch Normalization and Dropout\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout with 30% rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        return self.relu(x + shortcut)\n",
    "\n",
    "# Define the complete ResNet architecture with deeper layers\n",
    "class DeepResNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeepResNet, self).__init__()\n",
    "        self.fc_initial = nn.Linear(input_size, 512)\n",
    "        self.block1 = ResNetBlock(512)\n",
    "        self.block2 = ResNetBlock(512)\n",
    "        self.block3 = ResNetBlock(512)  # Additional ResNet block\n",
    "        self.block4 = ResNetBlock(512)  # Additional ResNet block\n",
    "        self.fc_final = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with 50% rate for the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc_initial(x))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc_final(x))\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(n):\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32  # You can adjust this to any suitable size\n",
    "\n",
    "# Train the DeepResNet model with mini-batch gradient descent and early stopping\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size, num_epochs=500, learning_rate=0.001, patience=10):\n",
    "    # Convert data to PyTorch tensors and create DataLoader for mini-batches\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # DataLoader for mini-batch gradient descent\n",
    "    \n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Initialize model, apply Xavier initialization, loss function, and optimizer\n",
    "    model = DeepResNet(input_size)\n",
    "    model.apply(init_weights_xavier)  # Apply Xavier initialization to all linear layers\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "    # Track losses and accuracies for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0  # To accumulate the training loss over all batches\n",
    "\n",
    "        # Loop over mini-batches\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            train_loss = criterion(output, y_batch)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += train_loss.item()  # Accumulate training loss\n",
    "\n",
    "        # Average training loss over all batches for the epoch\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation loss and accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "            val_pred = (val_output > 0.5).float()\n",
    "            val_accuracy = accuracy_score(y_val, val_pred.numpy())\n",
    "\n",
    "            # # Early stopping logic\n",
    "            # if val_loss < best_val_loss:\n",
    "            #     best_val_loss = val_loss\n",
    "            #     patience_counter = 0\n",
    "            # else:\n",
    "            #     patience_counter += 1\n",
    "            #     if patience_counter >= patience:\n",
    "            #         print(\"Early stopping triggered\")\n",
    "            #         break\n",
    "\n",
    "            # Record validation loss and accuracy\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_pred = (test_output > 0.5).float()\n",
    "        test_accuracy = accuracy_score(y_test, test_pred.numpy())\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    return test_accuracy\n",
    "\n",
    "# Plot losses and accuracies\n",
    "def plot_metrics(train_losses, val_losses, val_accuracies, dataset_name):\n",
    "    epochs_train = range(1, len(train_losses) + 1)\n",
    "    epochs_val = range(1, len(val_losses) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    ax1.plot(epochs_train, train_losses, label=\"Train Loss\")\n",
    "    ax1.plot(epochs_val, val_losses, label=\"Validation Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"{dataset_name} Loss over Epochs\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Use the same length adjustment for validation accuracy if needed\n",
    "    epochs_val_acc = range(1, len(val_accuracies) + 1)\n",
    "    ax2.plot(epochs_val_acc, val_accuracies, label=\"Validation Accuracy\", color='green')\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(f\"{dataset_name} Validation Accuracy over Epochs\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "possible_n_vals = [24, 30, 45]\n",
    "for n in possible_n_vals:\n",
    "    print(f\"\\nDataset kryptonite-{n}\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(n)\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model, train_losses, val_losses, val_accuracies = train_model(X_train, y_train, X_val, y_val, input_size)\n",
    "    test_accuracy = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Plot metrics\n",
    "    plot_metrics(train_losses, val_losses, val_accuracies, f\"Dataset kryptonite-{n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset kryptonite-24\n",
      "Epoch [10/500], Train Loss: 0.7168, Val Loss: 0.7314, Val Accuracy: 0.5009\n",
      "Epoch [20/500], Train Loss: 0.7033, Val Loss: 0.7229, Val Accuracy: 0.5006\n",
      "Epoch [30/500], Train Loss: 0.6937, Val Loss: 0.6987, Val Accuracy: 0.5000\n",
      "Epoch [40/500], Train Loss: 0.6864, Val Loss: 0.7029, Val Accuracy: 0.5055\n",
      "Epoch [50/500], Train Loss: 0.6712, Val Loss: 0.7094, Val Accuracy: 0.5017\n",
      "Epoch [60/500], Train Loss: 0.6369, Val Loss: 0.7596, Val Accuracy: 0.4992\n",
      "Epoch [70/500], Train Loss: 0.6013, Val Loss: 0.8230, Val Accuracy: 0.4978\n",
      "Epoch [80/500], Train Loss: 0.5394, Val Loss: 0.9284, Val Accuracy: 0.4978\n",
      "Epoch [90/500], Train Loss: 0.4854, Val Loss: 0.9892, Val Accuracy: 0.4996\n",
      "Epoch [100/500], Train Loss: 0.4419, Val Loss: 1.0999, Val Accuracy: 0.5009\n",
      "Epoch [110/500], Train Loss: 0.4003, Val Loss: 1.1068, Val Accuracy: 0.4962\n",
      "Epoch [120/500], Train Loss: 0.3711, Val Loss: 1.2260, Val Accuracy: 0.4958\n",
      "Epoch [130/500], Train Loss: 0.3411, Val Loss: 1.3688, Val Accuracy: 0.4975\n",
      "Epoch [140/500], Train Loss: 0.3206, Val Loss: 1.4606, Val Accuracy: 0.4960\n",
      "Epoch [150/500], Train Loss: 0.3110, Val Loss: 1.3984, Val Accuracy: 0.4945\n",
      "Epoch [160/500], Train Loss: 0.2931, Val Loss: 1.5419, Val Accuracy: 0.5003\n",
      "Epoch [170/500], Train Loss: 0.2867, Val Loss: 1.5080, Val Accuracy: 0.4988\n",
      "Epoch [180/500], Train Loss: 0.2625, Val Loss: 1.6270, Val Accuracy: 0.4978\n",
      "Epoch [190/500], Train Loss: 0.2638, Val Loss: 1.6011, Val Accuracy: 0.4918\n",
      "Epoch [200/500], Train Loss: 0.2586, Val Loss: 1.6634, Val Accuracy: 0.4958\n",
      "Epoch [210/500], Train Loss: 0.2427, Val Loss: 1.8012, Val Accuracy: 0.4919\n",
      "Epoch [220/500], Train Loss: 0.2402, Val Loss: 1.8466, Val Accuracy: 0.4963\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "# Xavier initialization function\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# ResNet Block with Batch Normalization, Dropout, and Stochastic Depth\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_size, stochastic_depth_prob=0.0):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.stochastic_depth_prob = stochastic_depth_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and random.random() < self.stochastic_depth_prob:\n",
    "            return x  # Skip this block\n",
    "        else:\n",
    "            shortcut = x\n",
    "            x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = self.bn2(self.fc2(x))\n",
    "            return self.leaky_relu(x + shortcut)\n",
    "\n",
    "# Deep ResNet architecture with additional ResNet blocks\n",
    "class DeepResNet(nn.Module):\n",
    "    def __init__(self, input_size, stochastic_depth_prob=0.2):\n",
    "        super(DeepResNet, self).__init__()\n",
    "        self.fc_initial = nn.Linear(input_size, 1024)  # Increased to 1024 units\n",
    "        self.block1 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block2 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block3 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block4 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block5 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)  # Additional block\n",
    "        self.block6 = ResNetBlock(1024, stochastic_depth_prob=stochastic_depth_prob)  # Additional block\n",
    "        self.fc_final = nn.Linear(1024, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc_initial(x))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_final(x)  # No sigmoid here since BCEWithLogitsLoss includes it\n",
    "        return x\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(n):\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train the DeepResNet model with mini-batch gradient descent and early stopping\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size, num_epochs=500, learning_rate=0.001, patience=10):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model = DeepResNet(input_size)\n",
    "    model.apply(init_weights_xavier)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Updated to BCEWithLogitsLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)  # Reduced weight decay\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            train_loss = criterion(output, y_batch)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += train_loss.item()\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "            val_pred = (torch.sigmoid(val_output) > 0.5).float()  # Apply sigmoid here for accuracy calculation\n",
    "            val_accuracy = accuracy_score(y_val, val_pred.numpy())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Evaluate model on test set\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_pred = (torch.sigmoid(test_output) > 0.5).float()  # Apply sigmoid for final output thresholding\n",
    "        test_accuracy = accuracy_score(y_test, test_pred.numpy())\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    return test_accuracy\n",
    "\n",
    "# Plot losses and accuracies\n",
    "def plot_metrics(train_losses, val_losses, val_accuracies, dataset_name):\n",
    "    epochs_train = range(1, len(train_losses) + 1)\n",
    "    epochs_val = range(1, len(val_losses) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.plot(epochs_train, train_losses, label=\"Train Loss\")\n",
    "    ax1.plot(epochs_val, val_losses, label=\"Validation Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"{dataset_name} Loss over Epochs\")\n",
    "    ax1.legend()\n",
    "\n",
    "    epochs_val_acc = range(1, len(val_accuracies) + 1)\n",
    "    ax2.plot(epochs_val_acc, val_accuracies, label=\"Validation Accuracy\", color='green')\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(f\"{dataset_name} Validation Accuracy over Epochs\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "possible_n_vals = [24, 30, 45]\n",
    "for n in possible_n_vals:\n",
    "    print(f\"\\nDataset kryptonite-{n}\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(n)\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    model, train_losses, val_losses, val_accuracies = train_model(X_train, y_train, X_val, y_val, input_size)\n",
    "    test_accuracy = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, val_accuracies, f\"Dataset kryptonite-{n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset kryptonite-24\n",
      "Epoch [10/500], Train Loss: 0.6935, Val Loss: 0.6950, Val Accuracy: 0.4999\n",
      "Epoch [20/500], Train Loss: 0.6928, Val Loss: 0.6945, Val Accuracy: 0.5038\n",
      "Epoch [30/500], Train Loss: 0.6923, Val Loss: 0.6941, Val Accuracy: 0.4994\n",
      "Epoch [40/500], Train Loss: 0.6907, Val Loss: 0.6944, Val Accuracy: 0.5033\n",
      "Epoch [50/500], Train Loss: 0.6907, Val Loss: 0.6949, Val Accuracy: 0.5047\n",
      "Epoch [60/500], Train Loss: 0.6900, Val Loss: 0.6960, Val Accuracy: 0.4967\n",
      "Epoch [70/500], Train Loss: 0.6886, Val Loss: 0.6973, Val Accuracy: 0.4972\n",
      "Epoch [80/500], Train Loss: 0.6878, Val Loss: 0.6975, Val Accuracy: 0.4933\n",
      "Epoch [90/500], Train Loss: 0.6859, Val Loss: 0.7001, Val Accuracy: 0.4992\n",
      "Epoch [100/500], Train Loss: 0.6837, Val Loss: 0.6993, Val Accuracy: 0.5039\n",
      "Epoch [110/500], Train Loss: 0.6836, Val Loss: 0.6990, Val Accuracy: 0.5002\n",
      "Epoch [120/500], Train Loss: 0.6812, Val Loss: 0.7035, Val Accuracy: 0.5003\n",
      "Epoch [130/500], Train Loss: 0.6800, Val Loss: 0.7068, Val Accuracy: 0.5027\n",
      "Epoch [140/500], Train Loss: 0.6784, Val Loss: 0.7105, Val Accuracy: 0.4987\n",
      "Epoch [150/500], Train Loss: 0.6783, Val Loss: 0.7058, Val Accuracy: 0.5023\n",
      "Epoch [160/500], Train Loss: 0.6773, Val Loss: 0.7056, Val Accuracy: 0.5023\n",
      "Epoch [170/500], Train Loss: 0.6749, Val Loss: 0.7058, Val Accuracy: 0.4967\n",
      "Epoch [180/500], Train Loss: 0.6739, Val Loss: 0.7083, Val Accuracy: 0.4983\n",
      "Epoch [190/500], Train Loss: 0.6737, Val Loss: 0.7127, Val Accuracy: 0.5022\n",
      "Epoch [200/500], Train Loss: 0.6726, Val Loss: 0.7112, Val Accuracy: 0.5026\n",
      "Epoch [210/500], Train Loss: 0.6707, Val Loss: 0.7148, Val Accuracy: 0.5007\n",
      "Epoch [220/500], Train Loss: 0.6688, Val Loss: 0.7137, Val Accuracy: 0.5044\n",
      "Epoch [230/500], Train Loss: 0.6658, Val Loss: 0.7158, Val Accuracy: 0.5022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 180\u001b[0m\n\u001b[1;32m    177\u001b[0m X_train, X_val, X_test, y_train, y_val, y_test \u001b[38;5;241m=\u001b[39m load_data(n)\n\u001b[1;32m    178\u001b[0m input_size \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 180\u001b[0m model, train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m train_model(X_train, y_train, X_val, y_val, input_size)\n\u001b[1;32m    181\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_test, y_test)\n\u001b[1;32m    183\u001b[0m plot_metrics(train_losses, val_losses, val_accuracies, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset kryptonite-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 114\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_val, y_val, input_size, num_epochs, learning_rate, patience)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    113\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m    115\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m    116\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36mDeepResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_initial(x))\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1(x)\n\u001b[0;32m---> 59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\n\u001b[1;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3(x)\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mResNetBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "# Xavier initialization function\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# ResNet Block with Batch Normalization, Dropout, and Stochastic Depth\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_size, stochastic_depth_prob=0.0):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.stochastic_depth_prob = stochastic_depth_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and random.random() < self.stochastic_depth_prob:\n",
    "            return x  # Skip this block\n",
    "        else:\n",
    "            shortcut = x\n",
    "            x = self.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = self.bn2(self.fc2(x))\n",
    "            return self.relu(x + shortcut)\n",
    "\n",
    "# Deep ResNet architecture with additional ResNet blocks\n",
    "class DeepResNet(nn.Module):\n",
    "    def __init__(self, input_size, stochastic_depth_prob=0.2):\n",
    "        super(DeepResNet, self).__init__()\n",
    "        self.fc_initial = nn.Linear(input_size, 512)\n",
    "        self.block1 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block2 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block3 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block4 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)\n",
    "        self.block5 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)  # Additional block\n",
    "        self.block6 = ResNetBlock(512, stochastic_depth_prob=stochastic_depth_prob)  # Additional block\n",
    "        self.fc_final = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc_initial(x))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc_final(x))\n",
    "        return x\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(n):\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train the DeepResNet model with mini-batch gradient descent and early stopping\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size, num_epochs=500, learning_rate=0.001, patience=10):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model = DeepResNet(input_size)\n",
    "    model.apply(init_weights_xavier)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            train_loss = criterion(output, y_batch)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += train_loss.item()\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val_tensor)\n",
    "            val_loss = criterion(val_output, y_val_tensor)\n",
    "            val_pred = (val_output > 0.5).float()\n",
    "            val_accuracy = accuracy_score(y_val, val_pred.numpy())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Evaluate model on test set\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_pred = (test_output > 0.5).float()\n",
    "        test_accuracy = accuracy_score(y_test, test_pred.numpy())\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    return test_accuracy\n",
    "\n",
    "# Plot losses and accuracies\n",
    "def plot_metrics(train_losses, val_losses, val_accuracies, dataset_name):\n",
    "    epochs_train = range(1, len(train_losses) + 1)\n",
    "    epochs_val = range(1, len(val_losses) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.plot(epochs_train, train_losses, label=\"Train Loss\")\n",
    "    ax1.plot(epochs_val, val_losses, label=\"Validation Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"{dataset_name} Loss over Epochs\")\n",
    "    ax1.legend()\n",
    "\n",
    "    epochs_val_acc = range(1, len(val_accuracies) + 1)\n",
    "    ax2.plot(epochs_val_acc, val_accuracies, label=\"Validation Accuracy\", color='green')\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(f\"{dataset_name} Validation Accuracy over Epochs\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "possible_n_vals = [24, 30, 45]\n",
    "for n in possible_n_vals:\n",
    "    print(f\"\\nDataset kryptonite-{n}\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(n)\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    model, train_losses, val_losses, val_accuracies = train_model(X_train, y_train, X_val, y_val, input_size)\n",
    "    test_accuracy = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, val_accuracies, f\"Dataset kryptonite-{n}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
