{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the sample mean deviates from the true mean by more than 0.05: 1.2130613194252666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the Hoeffding bound\n",
    "def hoeffding_bound(n, epsilon):\n",
    "    return 2 * np.exp(-2 * n * epsilon**2)\n",
    "\n",
    "# Sample usage\n",
    "n = 100  # sample size\n",
    "epsilon = 0.05  # desired deviation from true mean\n",
    "confidence_bound = hoeffding_bound(n, epsilon)\n",
    "print(f\"Probability that the sample mean deviates from the true mean by more than {epsilon}: {confidence_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size for epsilon = 0.05 and delta = 0.01: 1060\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function to calculate minimum sample size for desired epsilon and delta\n",
    "def required_sample_size(epsilon, delta):\n",
    "    return math.ceil((1 / (2 * epsilon**2)) * np.log(2 / delta))\n",
    "\n",
    "# Sample usage\n",
    "epsilon = 0.05  # maximum allowed deviation\n",
    "delta = 0.01    # confidence level (1 - delta)\n",
    "n_required = required_sample_size(epsilon, delta)\n",
    "print(f\"Required sample size for epsilon = {epsilon} and delta = {delta}: {n_required}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error bound with 3 models: 3.6391839582758\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate generalization error bound using Union Bound\n",
    "def generalization_error_bound(n, epsilon, num_models):\n",
    "    return 2 * num_models * np.exp(-2 * n * epsilon**2)\n",
    "\n",
    "# Sample usage\n",
    "n = 100  # sample size\n",
    "epsilon = 0.05  # deviation tolerance\n",
    "num_models = 3  # number of models in hypothesis space\n",
    "gen_error_bound = generalization_error_bound(n, epsilon, num_models)\n",
    "print(f\"Generalization error bound with {num_models} models: {gen_error_bound}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Cross-Validation Mean Accuracy: 0.9577777777777777\n",
      "Neural Network confidence bound with epsilon = 0.05: 3.0459959489425146e-08\n",
      "Extra Trees confidence bound with epsilon = 0.05: 1.9506198240566652\n",
      "Required sample size for epsilon = 0.05 and delta = 0.01: 1060\n",
      "Generalization error bound with 2 models: 6.091991897885029e-08\n",
      "Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96      1808\n",
      "         1.0       0.96      0.96      0.96      1792\n",
      "\n",
      "    accuracy                           0.96      3600\n",
      "   macro avg       0.96      0.96      0.96      3600\n",
      "weighted avg       0.96      0.96      0.96      3600\n",
      "\n",
      "Validation Accuracy of Best Neural Network Model: 0.9575\n",
      "Extra Trees Cross-Validation Classification Report:\n",
      "Fold 1 Accuracy: 0.9575\n",
      "Fold 2 Accuracy: 0.9613888888888888\n",
      "Fold 3 Accuracy: 0.9591666666666666\n",
      "Fold 4 Accuracy: 0.9580555555555555\n",
      "Fold 5 Accuracy: 0.9527777777777777\n",
      "Mean Accuracy of Extra Trees: 0.9577777777777777\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import math\n",
    "\n",
    "# Load the data\n",
    "X = np.load('Datasets/kryptonite-9-X.npy')\n",
    "y = np.load('Datasets/kryptonite-9-y.npy')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the entire dataset for polynomial feature transformation\n",
    "\n",
    "# Convert data to PyTorch tensors for the neural network\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "\n",
    "# Define Neural Network model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Neural Network\n",
    "num_epochs = 50\n",
    "best_val_accuracy = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_preds = (val_outputs > 0.5).float()\n",
    "        val_accuracy = accuracy_score(y_val_tensor, val_preds)\n",
    "        \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Extra Trees with Polynomial Features\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# K-Fold Cross-Validation for Extra Trees\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "et_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_poly):\n",
    "    X_train_fold, X_val_fold = X_poly[train_index], X_poly[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    et_model.fit(X_train_fold, y_train_fold)\n",
    "    y_val_pred = et_model.predict(X_val_fold)\n",
    "    fold_accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    et_accuracies.append(fold_accuracy)\n",
    "\n",
    "mean_et_accuracy = np.mean(et_accuracies)\n",
    "print(\"Extra Trees Cross-Validation Mean Accuracy:\", mean_et_accuracy)\n",
    "\n",
    "# Hoeffding's Inequality calculations\n",
    "\n",
    "# 1. Confidence Interval Bound\n",
    "def hoeffding_bound(n, epsilon):\n",
    "    return 2 * np.exp(-2 * n * epsilon**2)\n",
    "\n",
    "n_samples = len(y_val)  # For neural network validation set or k-fold for Extra Trees\n",
    "epsilon = 0.05  # Desired deviation from true mean\n",
    "\n",
    "confidence_bound_nn = hoeffding_bound(n_samples, epsilon)\n",
    "confidence_bound_et = hoeffding_bound(len(et_accuracies), epsilon)\n",
    "\n",
    "print(f\"Neural Network confidence bound with epsilon = {epsilon}: {confidence_bound_nn}\")\n",
    "print(f\"Extra Trees confidence bound with epsilon = {epsilon}: {confidence_bound_et}\")\n",
    "\n",
    "# 2. Sample Complexity Estimation\n",
    "def required_sample_size(epsilon, delta):\n",
    "    return math.ceil((1 / (2 * epsilon**2)) * np.log(2 / delta))\n",
    "\n",
    "# Define parameters for sample size estimation\n",
    "delta = 0.01  # Confidence level (1 - delta)\n",
    "n_required = required_sample_size(epsilon, delta)\n",
    "print(f\"Required sample size for epsilon = {epsilon} and delta = {delta}: {n_required}\")\n",
    "\n",
    "# 3. Generalization Error Bound with Union Bound\n",
    "def generalization_error_bound(n, epsilon, num_models):\n",
    "    return 2 * num_models * np.exp(-2 * n * epsilon**2)\n",
    "\n",
    "num_models = 2  # Number of models (Neural Network and Extra Trees)\n",
    "gen_error_bound = generalization_error_bound(n_samples, epsilon, num_models)\n",
    "print(f\"Generalization error bound with {num_models} models: {gen_error_bound}\")\n",
    "\n",
    "# Final Classification Reports for Best Models\n",
    "with torch.no_grad():\n",
    "    val_preds_nn = model(X_val_tensor)\n",
    "    val_preds_nn = (val_preds_nn > 0.5).float()\n",
    "    print(\"Neural Network Classification Report:\")\n",
    "    print(classification_report(y_val_tensor, val_preds_nn))\n",
    "    print(\"Validation Accuracy of Best Neural Network Model:\", accuracy_score(y_val, val_preds_nn))\n",
    "\n",
    "print(\"Extra Trees Cross-Validation Classification Report:\")\n",
    "for i, acc in enumerate(et_accuracies, 1):\n",
    "    print(f\"Fold {i} Accuracy: {acc}\")\n",
    "print(f\"Mean Accuracy of Extra Trees: {mean_et_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence bound with epsilon = 0.05 and n = 18000: 1.6388025247980798e-39\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples in the dataset\n",
    "n_samples = X.shape[0]\n",
    "epsilon = 0.05  # Desired deviation from the true mean\n",
    "\n",
    "# Hoeffding's Inequality confidence bound calculation\n",
    "def hoeffding_bound(n, epsilon):\n",
    "    return 2 * np.exp(-2 * n * epsilon**2)\n",
    "\n",
    "confidence_bound = hoeffding_bound(n_samples, epsilon)\n",
    "print(f\"Confidence bound with epsilon = {epsilon} and n = {n_samples}: {confidence_bound}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size for epsilon = 0.05 and delta = 0.01: 1060\n"
     ]
    }
   ],
   "source": [
    "def required_sample_size(epsilon, delta):\n",
    "    return math.ceil((1 / (2 * epsilon**2)) * np.log(2 / delta))\n",
    "\n",
    "# Define epsilon and delta for sample complexity estimation\n",
    "delta = 0.01  # Confidence level (1 - delta)\n",
    "n_required = required_sample_size(epsilon, delta)\n",
    "print(f\"Required sample size for epsilon = {epsilon} and delta = {delta}: {n_required}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
