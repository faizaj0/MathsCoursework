{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the basic methods that can potentially reduce the energy and resource consumption of ML models without significantly compromising performance. Here are the implementation examples of these methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEMO PURPOSES, REPLACE WITH YOUR OWN MODEL CODE\n",
    "n = 9\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load and preprocess data\n",
    "X = np.load(f'../Datasets/kryptonite-{n}-X.npy')\n",
    "y = np.load(f'../Datasets/kryptonite-{n}-y.npy')\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class\n",
    "class KryptoniteDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# Model\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, num_layers=6, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial linear projection to match transformer input requirements\n",
    "        self.input_proj = nn.Linear(input_dim, dim_feedforward)\n",
    "        \n",
    "        # Position encoding (learned)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 1, dim_feedforward))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward*2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim_feedforward, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add sequence dimension\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, criterion, optimizer, etc.\n",
    "model = TransformerClassifier(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = KryptoniteDataset(X_train, y_train)\n",
    "val_dataset = KryptoniteDataset(X_val, y_val)\n",
    "test_dataset = KryptoniteDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Early Stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping terminates training when the model's performance on a validation set stops improving. This avoids unnecessary epochs, reducing time and energy usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, y_batch).item()\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dynamic Learning Rate (LR) Scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the learning rate as training progresses, you can often reach convergence with fewer epochs. This technique helps avoid excess computation in later epochs and reduces energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# During training\n",
    "for epoch in range(epochs):\n",
    "    # Train and validate as before\n",
    "    # ...\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mixed Precision Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision allows the model to use both 32-bit and 16-bit floating-point operations. This can significantly reduce memory usage and increase computational speed without much performance loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Scale the loss and backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pruning (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pruning reduces the size of a network by removing less important weights, thus reducing memory usage and computation. Techniques like structured pruning remove entire neurons or channels rather than individual weights, which is more efficient in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ResNet\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Prune 20% of the neurons in conv1 layer by L1 norm\n",
    "prune.ln_structured(model.resnet.conv1, name=\"weight\", amount=0.2, n=1, dim=0)\n",
    "\n",
    "# Remove pruning reparameterization to finalize the model\n",
    "prune.remove(model.resnet.conv1, 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For basic NeuralNet\n",
    "\n",
    "# Apply pruning to the first fully connected layer (fc1)\n",
    "prune.l1_unstructured(model.fc1, name=\"weight\", amount=0.3)  # Prune 30% of weights\n",
    "\n",
    "# Optionally remove the pruning reparameterization\n",
    "prune.remove(model.fc1, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For basic Decision tree\n",
    "\n",
    "# Pruning logic for decision trees\n",
    "def prune_decision_tree(tree_model, importance_threshold):\n",
    "    # Example: Prune nodes with weights below a threshold\n",
    "    for name, param in tree_model.named_parameters():\n",
    "        if 'weight' in name and torch.abs(param).mean() < importance_threshold:\n",
    "            print(f\"Pruning node: {name}\")\n",
    "            param.data.fill_(0)  # Remove the contribution of this node\n",
    "\n",
    "# Apply pruning\n",
    "prune_decision_tree(tree_model, importance_threshold=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
